{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cda048d-84da-4c00-8be9-80c2f5040e1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'homophones'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-447ce8aa38c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhomophones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'homophones'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from difflib import SequenceMatcher\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import homophones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe7129-76db-40c4-baa0-eca3c6b748f8",
   "metadata": {},
   "source": [
    "# homophone.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03ff1fc9-121e-4e05-9356-5cd5a847df99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python wrapper for the website: https://www.homophone.com/\n",
    "Gets the homophones of a word.\n",
    "\"\"\"\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List\n",
    "import re\n",
    "\n",
    "class Pyphones:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.url = \"https://www.homophone.com/search?page={}&type=&q={}\"\n",
    "        self.homophones = {self.word: []}\n",
    "        \n",
    "    def get_the_page(self, page_no = 1):\n",
    "        \"\"\"\n",
    "        Get the page content.\n",
    "\n",
    "        Returns\n",
    "            str: the content of the page.\n",
    "        \"\"\"\n",
    "        url = self.url.format(page_no, self.word)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        return soup\n",
    "\n",
    "    def get_the_page_nos(self):\n",
    "        \"\"\"\n",
    "        Get the total number of pages\n",
    "\n",
    "        Returns\n",
    "            int: the total number of the pages.\n",
    "        \"\"\"\n",
    "        soup = self.get_the_page()\n",
    "        pages = soup.find_all('div', attrs={'class':'col-sm-9'})\n",
    "        total_pages = pages[0].find('h5').text.split('/')[-1].strip()\n",
    "        return int(total_pages)\n",
    "\n",
    "    def get_the_homophones(self):\n",
    "        \"\"\"\n",
    "        Get the homophones of the word.\n",
    "\n",
    "        Returns\n",
    "            dict: {word: [list_of_homophones]} against each word.\n",
    "        \"\"\"\n",
    "        total_pages = self.get_the_page_nos()\n",
    "        for ix in range(total_pages):\n",
    "            page_no = ix + 1\n",
    "            soup = self.get_the_page(page_no)\n",
    "            raw_homophones = soup.find_all('div', attrs={'class': 'well well-lg'})\n",
    "            for elem in range(len(raw_homophones)):\n",
    "                raw_homophones_2 = raw_homophones[elem].find_all('a', attrs={'class': 'btn word-btn'})\n",
    "                list_of_homophones = list(raw_homophones_2)\n",
    "                if any(list_of_homophones):\n",
    "                    local_homophones = []\n",
    "                    for tag_of_homophone in list_of_homophones:\n",
    "                        homophone = tag_of_homophone.text\n",
    "                        local_homophones.append(homophone)\n",
    "                    self.homophones[self.word].append(local_homophones)\n",
    "\n",
    "        return self.homophones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dfa7247-fc6d-446f-a95a-199bf4d70f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\LG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# CMU Dictionary\n",
    "nltk.download('cmudict')\n",
    "cmu = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a907443-47ea-416b-8038-4e4602394e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a34490-75e3-4937-b11b-da227be2923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c19887-62f2-425a-a340-1a35dfac2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update('.', '?', '-', '\\'', '\\:', ',', '!', '<', '>', '\\\"', '/', '(', ')',\n",
    "                  '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 's', 't', 're', 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c072bc7d-c73b-40c5-92f3-4c64e9b7b290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'was', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "text = \"The quick brown fox was jumping over the lazy dog.\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18be89d-cbde-49e8-b318-23710c254324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10557d10-2c6b-4e58-86ba-bb51fe8cf0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove Punctuation and Tokenize Text\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "#     words = tokenizer.tokenize(text)\n",
    "    words = text.split()\n",
    "    original_sentences = []\n",
    "    original_sentence = []\n",
    "    \n",
    "    vocab = w2v_model.index_to_key\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        original_sentence.append(words[i].lower())\n",
    "    original_sentences.append(original_sentence)\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() not in stop_words and words[i].lower() in vocab:\n",
    "            sentence.append(words[i].lower())\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "    print('original_sentences',original_sentences[0])\n",
    "    print(\" \")\n",
    "    print('sentences',sentences[0])\n",
    "    \n",
    "    return sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c704970d-5741-416d-8e6e-7512ab5abeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_sentences ['the', 'boating', 'store', 'had', 'its', 'best', 'sail', 'ever']\n",
      " \n",
      "sentences ['boating', 'store', 'best', 'sail', 'ever']\n",
      "word boating\n",
      "cmu_dict[word] [['B', 'OW1', 'T', 'IH0', 'NG']]\n",
      "key boating\n",
      "value [['B', 'OW1', 'T', 'IH0', 'NG']]\n",
      "\n",
      "Homophones for \"boating\": ['boating']\n",
      "word store\n",
      "cmu_dict[word] [['S', 'T', 'AO1', 'R']]\n",
      "key stoehr\n",
      "value [['S', 'T', 'AO1', 'R']]\n",
      "\n",
      "word store\n",
      "cmu_dict[word] [['S', 'T', 'AO1', 'R']]\n",
      "key stohr\n",
      "value [['S', 'T', 'AO1', 'R']]\n",
      "\n",
      "word store\n",
      "cmu_dict[word] [['S', 'T', 'AO1', 'R']]\n",
      "key store\n",
      "value [['S', 'T', 'AO1', 'R']]\n",
      "\n",
      "word store\n",
      "cmu_dict[word] [['S', 'T', 'AO1', 'R']]\n",
      "key storr\n",
      "value [['S', 'T', 'AO1', 'R']]\n",
      "\n",
      "Homophones for \"store\": ['stoehr', 'stohr', 'store', 'storr']\n",
      "word best\n",
      "cmu_dict[word] [['B', 'EH1', 'S', 'T']]\n",
      "key best\n",
      "value [['B', 'EH1', 'S', 'T']]\n",
      "\n",
      "word best\n",
      "cmu_dict[word] [['B', 'EH1', 'S', 'T']]\n",
      "key beste\n",
      "value [['B', 'EH1', 'S', 'T']]\n",
      "\n",
      "Homophones for \"best\": ['best', 'beste']\n",
      "word sail\n",
      "cmu_dict[word] [['S', 'EY1', 'L']]\n",
      "key sail\n",
      "value [['S', 'EY1', 'L']]\n",
      "\n",
      "word sail\n",
      "cmu_dict[word] [['S', 'EY1', 'L']]\n",
      "key sale\n",
      "value [['S', 'EY1', 'L']]\n",
      "\n",
      "word sail\n",
      "cmu_dict[word] [['S', 'EY1', 'L']]\n",
      "key salle\n",
      "value [['S', 'EY1', 'L']]\n",
      "\n",
      "word sail\n",
      "cmu_dict[word] [['S', 'EY1', 'L']]\n",
      "key sayle\n",
      "value [['S', 'EY1', 'L']]\n",
      "\n",
      "Homophones for \"sail\": ['sail', 'sale', 'salle', 'sayle']\n",
      "word ever\n",
      "cmu_dict[word] [['EH1', 'V', 'ER0']]\n",
      "key ever\n",
      "value [['EH1', 'V', 'ER0']]\n",
      "\n",
      "Homophones for \"ever\": ['ever']\n",
      "temp_homophones ['stoehr', 'stohr', 'storr', 'beste', 'sale', 'salle', 'sayle']\n",
      "scores {'boating-beste': -0.061101273, 'boating-sale': 0.079889745, 'boating-salle': -0.01565561, 'store-beste': 0.0021596113, 'store-sale': 0.24029954, 'store-salle': 0.085700296, 'best-beste': 0.12706497, 'best-sale': 0.0643338, 'best-salle': -0.012982696, 'sail-sale': 0.053897303, 'sail-salle': 0.06103406}\n",
      "\n",
      "final [(0.24029954, 'store-sale'), (0.12706497, 'best-beste'), (0.085700296, 'store-salle'), (-0.061101273, 'boating-beste'), (-0.01565561, 'boating-salle'), (-0.012982696, 'best-salle')]\n",
      "\n",
      "top3 [(0.24029954, 'store-sale'), (0.12706497, 'best-beste'), (0.085700296, 'store-salle')]\n",
      " \n",
      "poss [['store', 'sale'], ['best', 'beste'], ['store', 'salle']]\n",
      " \n",
      "possible_pun_words {'beste', 'best', 'store', 'sale', 'salle'}\n",
      " \n",
      "Pun Keywords: {'beste', 'best', 'store', 'sale', 'salle'}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Find Homophones\n",
    "def find_homophones(word, cmu_dict):\n",
    "    \n",
    "    homophones = [key for key, value in cmu_dict.items() if value == cmu_dict[word]]\n",
    "#     temp_homophones = []\n",
    "    \n",
    "    for key, value in cmu_dict.items():\n",
    "        \n",
    "        if value == cmu_dict[word]:\n",
    "            print('word', word)\n",
    "            print('cmu_dict[word]', cmu_dict[word])\n",
    "            print('key', key)\n",
    "            print('value', value)\n",
    "            print(\"\")\n",
    "#             temp_homophones.append(key)\n",
    "#     print('temp_homophones', temp_homophones)\n",
    "    return homophones\n",
    "\n",
    "# Step 4: Phonetic Analysis\n",
    "def phonetic_analysis(word, cmu_dict):\n",
    "    return cmu_dict[word]\n",
    "\n",
    "# Step 5: Find Pun Keywords\n",
    "def find_pun_keywords(words, temp_homophones):\n",
    "    # Implement your pun keyword detection logic here\n",
    "    vocab = w2v_model.index_to_key\n",
    "    scores = {}\n",
    "\n",
    "    for i in range(len(words)-1):\n",
    "        for j in range(i+1, len(temp_homophones)):\n",
    "            if temp_homophones[j] not in vocab:\n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                sim_score = w2v_model.similarity(words[i], temp_homophones[j])\n",
    "                scores['{0}-{1}'.format(words[i], temp_homophones[j])] = sim_score\n",
    "    print('scores',scores)\n",
    "    if len(scores) >= 5:\n",
    "        top3 = sorted(zip(scores.values(), scores.keys()), reverse=True)[0:3]\n",
    "        bottom3 = sorted(zip(scores.values(), scores.keys()), reverse=False)[:3]\n",
    "        final = top3+bottom3\n",
    "#             final = list(final)\n",
    "#             final = tuple(final)\n",
    "#             print('')\n",
    "#             print('bottom3',bottom3)\n",
    "        print('')\n",
    "        print('final',final)\n",
    "        print('')\n",
    "        print('top3',top3)\n",
    "        print(\" \")\n",
    "        poss = [tup[1].split(sep='-') for tup in top3]\n",
    "#             poss = [tup[1].split(sep='-') for tup in final]\n",
    "\n",
    "        print('poss',poss)\n",
    "        print(\" \")\n",
    "        possible_pun_words = set(poss[0] + poss[1] + poss[2])\n",
    "#             possible_pun_words = set(poss[0] + poss[1] + poss[2]+poss[3]+poss[4]+poss[5])\n",
    "\n",
    "        print('possible_pun_words',possible_pun_words)\n",
    "        print(\" \")\n",
    "    else:\n",
    "        poss = [pair.split(sep='-') for pair in scores.keys()]\n",
    "        possible_pun_words = set()\n",
    "        for i in range(len(poss)):\n",
    "            possible_pun_words = possible_pun_words.union(set(poss[i]))\n",
    "        \"\"\"\n",
    "        top = sorted(zip(scores.values(), scores.keys()), reverse=True)[:1]\n",
    "        poss = [tup[1].split(sep='-') for tup in top]\n",
    "        possible_pun_words = set(poss[0])\n",
    "        \"\"\"\n",
    "\n",
    "    return possible_pun_words\n",
    "#     return []\n",
    "\n",
    "# Step 6: Search for Phrases\n",
    "def search_phrases(word, phrases, error_margin):\n",
    "    similar_phrases = []\n",
    "    for phrase in phrases:\n",
    "        similarity = SequenceMatcher(None, word, phrase).ratio()\n",
    "        print('word: ', word)\n",
    "        print('phrase:', phrase)\n",
    "        print('similarity', similarity)\n",
    "        print(\" \")\n",
    "        if similarity >= (1 - error_margin):\n",
    "            similar_phrases.append(phrase)\n",
    "    return similar_phrases\n",
    "\n",
    "# Example Usage\n",
    "text = \"The quick brown fox was jumping over the lazy dog.\"\n",
    "text2 = \"The boating store had its best sail ever.\"\n",
    "text3 = \"Authors can be very PENsive\"\n",
    "text4= \"I lift weights only on Saturday and Sunday because Monday to Friday are weak days.\"\n",
    "text5= 'why did the cookie cry, because his mother is awafer too long.'\n",
    "text6= 'The postmen get together for mail bonding .'\n",
    "text7= 'People who like gold paint have a gilt complex'\n",
    "text8 = 'a meowntain is a pile of kittens'\n",
    "\n",
    "# what do you call a pile of kittens? A meowntain\n",
    "\n",
    "# text = remove_punctuation(text)\n",
    "\n",
    "words = tokenize_text(text2)\n",
    "\n",
    "temp_homophones = []\n",
    "\n",
    "for word in words:\n",
    "    homophones = find_homophones(word, cmu)\n",
    "#     temp_homophones.append(homophones)\n",
    "    if homophones:\n",
    "        print(f'Homophones for \"{word}\": {homophones}')\n",
    "    else:\n",
    "        phonetics = phonetic_analysis(word, cmu)\n",
    "        print(f'Phonetics for \"{word}\": {phonetics}')\n",
    "    \n",
    "    for k in homophones:\n",
    "        if k in words:\n",
    "            pass\n",
    "        else:\n",
    "            temp_homophones.append(k)\n",
    "\n",
    "print('temp_homophones', temp_homophones)\n",
    "# Find Pun Keywords\n",
    "pun_keywords = find_pun_keywords(words, temp_homophones)\n",
    "print(f'Pun Keywords: {pun_keywords}')\n",
    "\n",
    "# Search for Phrases\n",
    "# phrases = [\"The quick brown fox\", \"The lazy dog\", \"jumps over\", \"was jumping\"]\n",
    "# error_margin = 0.2\n",
    "# for word in words:\n",
    "#     similar_phrases = search_phrases(word, phrases, error_margin)\n",
    "#     if similar_phrases:\n",
    "#         print(f'Similar Phrases for \"{word}\": {similar_phrases}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fcd8b-e6db-4f33-9d9f-a53424f427bd",
   "metadata": {},
   "source": [
    "you can ignore the number\n",
    "\n",
    "strsimpy - \n",
    "metaphone phonetic algorithm\n",
    "levenshtein distance algorithm\n",
    "\n",
    "\n",
    "How do you measure the distance if the word is not in CMU dictionary <= Using table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "389f8f08-0788-4c4e-b842-066e034cb852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24029954\n"
     ]
    }
   ],
   "source": [
    "sim_score = w2v_model.similarity('store', 'sale')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d7f7592-12d3-496a-a5ab-dfbdc380bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phonetic_analysis(word, cmu_dict):\n",
    "    return cmu_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d67beeb3-943d-4d2e-b6c2-647ec2368ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['S', 'EY1', 'L']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_analysis('sale', cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f309b821-13ea-4633-b18c-670ad5328b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['S', 'EY1', 'L']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_analysis('sail', cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "291e5ba4-ed4f-4a89-a21a-ec68b42eaa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['S', 'EY1', 'L']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_analysis('salle', cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddf5600c-7671-4413-8ce0-28287bb47faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['S', 'EY1', 'L']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_analysis('sayle', cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a1fdc9a-96f1-4d5c-adcb-c3c819ad25c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B', 'OW1', 'T', 'IH0', 'NG']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_analysis('boating', cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30c5da54-4833-49b9-8021-0f3648d5665f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['V', 'OW1', 'T', 'IH0', 'NG']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_analysis('voting', cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2dce9d57-701b-4fea-85ec-92de0b218063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['M', 'IY0', 'AW1']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic_analysis('meow', cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cad0c719-5bd9-4745-a372-ece164d36eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'why did the cookie cry, because his mother is awafer too long.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-85a3026e4f65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mphonetic_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'why did the cookie cry, because his mother is awafer too long.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-7349c6bb9493>\u001b[0m in \u001b[0;36mphonetic_analysis\u001b[1;34m(word, cmu_dict)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Step 4: Phonetic Analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mphonetic_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmu_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcmu_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Step 5: Find Pun Keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'why did the cookie cry, because his mother is awafer too long.'"
     ]
    }
   ],
   "source": [
    "phonetic_analysis('why did the cookie cry, because his mother is awafer too long.', cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e24f041-506f-4a94-9b12-aca3759c203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'boating', 'store', 'had', 'its', 'best', 'sail', 'ever', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenize_text('The boating store had its best sail ever.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cbff646-dfc1-409e-957e-b5b33448ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_sentences ['the', 'boating', 'store', 'had', 'its', 'best', 'sail', 'ever']\n",
      " \n",
      "sentences ['boating', 'store', 'best', 'sail', 'ever']\n",
      "word boating\n",
      "cmu_dict[word] [['B', 'OW1', 'T', 'IH0', 'NG']]\n",
      "key boating\n",
      "value [['B', 'OW1', 'T', 'IH0', 'NG']]\n",
      "\n",
      "Homophones for \"boating\": ['boating']\n",
      "word store\n",
      "cmu_dict[word] [['S', 'T', 'AO1', 'R']]\n",
      "key stoehr\n",
      "value [['S', 'T', 'AO1', 'R']]\n",
      "\n",
      "word store\n",
      "cmu_dict[word] [['S', 'T', 'AO1', 'R']]\n",
      "key stohr\n",
      "value [['S', 'T', 'AO1', 'R']]\n",
      "\n",
      "word store\n",
      "cmu_dict[word] [['S', 'T', 'AO1', 'R']]\n",
      "key store\n",
      "value [['S', 'T', 'AO1', 'R']]\n",
      "\n",
      "word store\n",
      "cmu_dict[word] [['S', 'T', 'AO1', 'R']]\n",
      "key storr\n",
      "value [['S', 'T', 'AO1', 'R']]\n",
      "\n",
      "Homophones for \"store\": ['stoehr', 'stohr', 'store', 'storr']\n",
      "word best\n",
      "cmu_dict[word] [['B', 'EH1', 'S', 'T']]\n",
      "key best\n",
      "value [['B', 'EH1', 'S', 'T']]\n",
      "\n",
      "word best\n",
      "cmu_dict[word] [['B', 'EH1', 'S', 'T']]\n",
      "key beste\n",
      "value [['B', 'EH1', 'S', 'T']]\n",
      "\n",
      "Homophones for \"best\": ['best', 'beste']\n",
      "word sail\n",
      "cmu_dict[word] [['S', 'EY1', 'L']]\n",
      "key sail\n",
      "value [['S', 'EY1', 'L']]\n",
      "\n",
      "word sail\n",
      "cmu_dict[word] [['S', 'EY1', 'L']]\n",
      "key sale\n",
      "value [['S', 'EY1', 'L']]\n",
      "\n",
      "word sail\n",
      "cmu_dict[word] [['S', 'EY1', 'L']]\n",
      "key salle\n",
      "value [['S', 'EY1', 'L']]\n",
      "\n",
      "word sail\n",
      "cmu_dict[word] [['S', 'EY1', 'L']]\n",
      "key sayle\n",
      "value [['S', 'EY1', 'L']]\n",
      "\n",
      "Homophones for \"sail\": ['sail', 'sale', 'salle', 'sayle']\n",
      "word ever\n",
      "cmu_dict[word] [['EH1', 'V', 'ER0']]\n",
      "key ever\n",
      "value [['EH1', 'V', 'ER0']]\n",
      "\n",
      "Homophones for \"ever\": ['ever']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"The boating store had its best sail ever.\"\n",
    "\n",
    "words = tokenize_text(text2)\n",
    "\n",
    "for word in words:\n",
    "#     print('word', word)\n",
    "    homophones = find_homophones(word, cmu)\n",
    "    if homophones:\n",
    "        print(f'Homophones for \"{word}\": {homophones}')\n",
    "    else:\n",
    "        phonetics = phonetic_analysis(word, cmu)\n",
    "        print(f'Phonetics for \"{word}\": {phonetics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44055743-164d-455b-9c9a-0eaacd3cb065",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'The boating store had its best sail ever'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-969790bd6d2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfind_homophones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The boating store had its best sail ever'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-604defc53846>\u001b[0m in \u001b[0;36mfind_homophones\u001b[1;34m(word, cmu_dict)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Step 3: Find Homophones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_homophones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmu_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mhomophones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcmu_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcmu_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhomophones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-604defc53846>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Step 3: Find Homophones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_homophones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmu_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mhomophones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcmu_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcmu_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhomophones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'The boating store had its best sail ever'"
     ]
    }
   ],
   "source": [
    "find_homophones('The boating store had its best sail ever',cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12eacce5-ca8a-4b75-a84c-0b0521552b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sail': [['sail', 'sale'], ['sailer', 'sailor'], ['sails', 'sales']]}\n"
     ]
    }
   ],
   "source": [
    "py = Pyphones(\"sail\")\n",
    "homophones = py.get_the_homophones()\n",
    "print(homophones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7ae2e-fa82-42e9-a020-f88e81242838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
