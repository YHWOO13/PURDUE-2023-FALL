{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "dfe37fb1-cb20-4891-8ed8-b43fd80c8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "from nltk import word_tokenize, MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "bef64106-ceaa-4b1f-a7a2-fde51cb54d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "82d15e56-62f2-4a55-a430-34f30a5f3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3630277f-3048-474f-80e2-54206eee6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update('.', '?', '-', '\\'', '\\:', ',', '!', '<', '>', '\\\"', '/', '(', ')',\n",
    "                  '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 's', 't', 're', 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "0abe7538-72f1-48d1-ae1d-418f92327d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4880635\n",
      "0.29449567\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.similarity('mouse', 'hamster'))\n",
    "print(w2v_model.similarity('mouse', 'owl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "d33793ec-9aea-4803-b7fa-1ee1aaa7b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "lst =[1,2,3,4,5,6,7,8,9]\n",
    "lst2 = (lst[0:3]+lst[-3:])\n",
    "print(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "7009175b-30f0-4bc7-bcbb-f6223adcf0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_pun_words(sent):\n",
    "    scores = {}\n",
    "    print('len',len(sent))\n",
    "    if len(sent) <= 1:\n",
    "        return set(sent[0])\n",
    "    else:\n",
    "        for i in range(len(sent)-1):\n",
    "            for j in range(i+1, len(sent)):\n",
    "                sim_score = w2v_model.similarity(sent[i], sent[j])\n",
    "                scores['{0}-{1}'.format(sent[i], sent[j])] = sim_score\n",
    "        print('scores',scores)\n",
    "        if len(scores) >= 5:\n",
    "            top3 = sorted(zip(scores.values(), scores.keys()), reverse=True)[0:3]\n",
    "            bottom3 = sorted(zip(scores.values(), scores.keys()), reverse=False)[:3]\n",
    "            final = top3+bottom3\n",
    "#             final = list(final)\n",
    "#             final = tuple(final)\n",
    "#             print('')\n",
    "#             print('bottom3',bottom3)\n",
    "#             print('')\n",
    "            print('final',final)\n",
    "            print('')\n",
    "            print('top3',top3)\n",
    "            print(\" \")\n",
    "            poss = [tup[1].split(sep='-') for tup in top3]\n",
    "#             poss = [tup[1].split(sep='-') for tup in final]\n",
    "\n",
    "            print('poss',poss)\n",
    "            print(\" \")\n",
    "            possible_pun_words = set(poss[0] + poss[1] + poss[2])\n",
    "#             possible_pun_words = set(poss[0] + poss[1] + poss[2]+poss[3]+poss[4]+poss[5])\n",
    "            \n",
    "            print('possible_pun_words',possible_pun_words)\n",
    "            print(\" \")\n",
    "        else:\n",
    "            poss = [pair.split(sep='-') for pair in scores.keys()]\n",
    "            possible_pun_words = set()\n",
    "            for i in range(len(poss)):\n",
    "                possible_pun_words = possible_pun_words.union(set(poss[i]))\n",
    "            \"\"\"\n",
    "            top = sorted(zip(scores.values(), scores.keys()), reverse=True)[:1]\n",
    "            poss = [tup[1].split(sep='-') for tup in top]\n",
    "            possible_pun_words = set(poss[0])\n",
    "            \"\"\"\n",
    "            \n",
    "        return possible_pun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "03a12643-cb0c-40ff-94de-7a961809c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pun_word(orig_sent, sent):\n",
    "    pun_words = get_possible_pun_words(sent)\n",
    "    pun_words = pun_words\n",
    "    largest_index = 0\n",
    "    print('orig_sent',orig_sent)\n",
    "    for w in pun_words:\n",
    "#         pun_word = w\n",
    "        index = orig_sent.index(w)\n",
    "        if index > largest_index:\n",
    "            largest_index = index\n",
    "#     return largest_index + 1\n",
    "    punword=orig_sent[largest_index+1]\n",
    "    print('lagrest_index',largest_index)\n",
    "#     print(orig_sent[13])\n",
    "    return punword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "360c764a-6e8b-4319-b789-ad556f090f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_predict(sentence):\n",
    " # Load pre-trained model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    words = tokenizer.convert_ids_to_tokens(encoding)\n",
    "    words = words [1:-1]\n",
    "    words = sentence.split()\n",
    "#     words = nlp(sentence)\n",
    "    original_sentences = []\n",
    "    original_sentence = []\n",
    "\n",
    "    results = []\n",
    "    print(words)\n",
    "    vocab = w2v_model.index_to_key\n",
    "\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        original_sentence.append(words[i].lower())\n",
    "    original_sentences.append(original_sentence)\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() not in stop_words and words[i].lower() in vocab:\n",
    "            sentence.append(words[i].lower())\n",
    "#             print(i, sentence)\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "    print('original_sentences',original_sentences[0])\n",
    "    print(\" \")\n",
    "    print('sentences',sentences[0])\n",
    "    \n",
    "    get_pun_words_list = []\n",
    "    text_list = []\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        get_pun_words_result = get_pun_word(original_sentences[i], sentences[i])\n",
    "        get_pun_words_list.append(get_pun_words_result)\n",
    "    print(get_pun_words_list)\n",
    "#     print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "987faa2a-c807-4b20-ae16-c70c48198e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I've got a bad case of shingles\"\n",
    "s2  = 'Autobiography when your car starts telling you about its life. '\n",
    "s3 = \"She was only a Blacksmith's daughter, but she knew how to forge ahead .\"\n",
    "s4 = \"A ditch digger was entrenched in his career .\"\n",
    "s5 =\"OLD GEOGRAPHERS never die , they just become legends .\"\n",
    "s6 =\"The young monkeys went to the jungle gym for some exercise\"\n",
    "s7 = \"The investor in the bakery demanded a larger piece of the pie.\"\n",
    "s8 = \"The art competition ended in a draw.\"\n",
    "s9 = \"The maestro turned away from the orchestra as they told him the bad news; he couldnâ€™t face the music.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "df53152c-cc04-451f-a305-3dc7f7d961be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OLD', 'GEOGRAPHERS', 'never', 'die', ',', 'they', 'just', 'become', 'legends', '.']\n",
      "original_sentences ['old', 'geographers', 'never', 'die', ',', 'they', 'just', 'become', 'legends', '.']\n",
      " \n",
      "sentences ['old', 'geographers', 'never', 'die', 'become', 'legends']\n",
      "len 6\n",
      "scores {'old-geographers': -0.026032887, 'old-never': 0.117951, 'old-die': 0.2354283, 'old-become': 0.097501636, 'old-legends': 0.086471416, 'geographers-never': 0.04967519, 'geographers-die': 0.046451222, 'geographers-become': 0.05736023, 'geographers-legends': 0.13373458, 'never-die': 0.20109296, 'never-become': 0.22373563, 'never-legends': 0.15837559, 'die-become': 0.21707109, 'die-legends': 0.08356403, 'become-legends': -0.01524009}\n",
      "final [(0.2354283, 'old-die'), (0.22373563, 'never-become'), (0.21707109, 'die-become'), (-0.026032887, 'old-geographers'), (-0.01524009, 'become-legends'), (0.046451222, 'geographers-die')]\n",
      "\n",
      "top3 [(0.2354283, 'old-die'), (0.22373563, 'never-become'), (0.21707109, 'die-become')]\n",
      " \n",
      "poss [['old', 'die'], ['never', 'become'], ['die', 'become']]\n",
      " \n",
      "possible_pun_words {'never', 'old', 'become', 'die'}\n",
      " \n",
      "orig_sent ['old', 'geographers', 'never', 'die', ',', 'they', 'just', 'become', 'legends', '.']\n",
      "lagrest_index 7\n",
      "['legends']\n"
     ]
    }
   ],
   "source": [
    "mask_and_predict(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "026c2ca4-2e28-40f7-9e2c-c45993694f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_with_most_senses(sentence):\n",
    "#     model_name = 'bert-base-uncased'\n",
    "#     tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#     encoding = tokenizer.encode(sentence)\n",
    "#     words = tokenizer.convert_ids_to_tokens(encoding)\n",
    "#     words = words [1:-1]\n",
    "    words = sentence.split()\n",
    "    print(words)\n",
    "#     text_id = words.attrib['id']\n",
    "#               words.index()\n",
    "    for i in range(len(words)):\n",
    "        senses_number = 0\n",
    "        prev_word_id = \"\"\n",
    "        if words[i].lower() not in stop_words:\n",
    "            word_senses_num = len(wn.synsets(words[i].lower()))\n",
    "            print(\" \")\n",
    "            print('wordsenses_num',word_senses_num)\n",
    "#             print('synsets', wn.synsets(child[i].text.lower()))\n",
    "            \n",
    "            print('synsets', wn.synsets(words[i].lower()))\n",
    "#             current_word_id = child[i].attrib['id']\n",
    "            current_word_id = words[i]\n",
    "            if word_senses_num >= senses_number:\n",
    "                senses_number = word_senses_num\n",
    "                prev_word_id = current_word_id\n",
    "    pun_prediction = prev_word_id\n",
    "    print(pun_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "a9df346e-a9d1-4082-ad00-2d3cae319de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OLD', 'GEOGRAPHERS', 'never', 'die', ',', 'they', 'just', 'become', 'legends', '.']\n",
      " \n",
      "wordsenses_num 9\n",
      "synsets [Synset('old.n.01'), Synset('old.a.01'), Synset('old.a.02'), Synset('old.s.03'), Synset('old.s.04'), Synset('erstwhile.s.01'), Synset('honest-to-god.s.01'), Synset('old.s.07'), Synset('previous.s.01')]\n",
      " \n",
      "wordsenses_num 1\n",
      "synsets [Synset('geographer.n.01')]\n",
      " \n",
      "wordsenses_num 2\n",
      "synsets [Synset('never.r.01'), Synset('never.r.02')]\n",
      " \n",
      "wordsenses_num 14\n",
      "synsets [Synset('die.n.01'), Synset('die.n.02'), Synset('die.n.03'), Synset('die.v.01'), Synset('die.v.02'), Synset('die.v.03'), Synset('fail.v.04'), Synset('die.v.05'), Synset('die.v.06'), Synset('die.v.07'), Synset('die.v.08'), Synset('die.v.09'), Synset('die.v.10'), Synset('die.v.11')]\n",
      " \n",
      "wordsenses_num 4\n",
      "synsets [Synset('become.v.01'), Synset('become.v.02'), Synset('become.v.03'), Synset('become.v.04')]\n",
      " \n",
      "wordsenses_num 2\n",
      "synsets [Synset('legend.n.01'), Synset('caption.n.03')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_word_with_most_senses(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ee358792-e7e4-4b06-9547-9bd29e800785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_definitions(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    definitions = [syn.definition() for syn in synsets]\n",
    "    return definitions\n",
    "\n",
    "def context_relevant_definition(word, context_word):\n",
    "    context_synsets = wn.synsets(context_word)\n",
    "    word_synsets = wn.synsets(word)\n",
    "\n",
    "    # If either word has no synsets, return None\n",
    "    if not context_synsets or not word_synsets:\n",
    "        return None\n",
    "\n",
    "    best_definition = None\n",
    "    max_similarity = -1\n",
    "\n",
    "    for w_syn in word_synsets:\n",
    "        for c_syn in context_synsets:\n",
    "            similarity = wn.wup_similarity(w_syn, c_syn)\n",
    "            if similarity is not None and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_definition = w_syn.definition()\n",
    "\n",
    "    return best_definition, max_similarity  # Return tuple with definition and similarity\n",
    "\n",
    "\n",
    "def compute_similarity(word1, word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "\n",
    "    # if there are no synsets, return 0\n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0\n",
    "\n",
    "    best_score = 0\n",
    "    # iterate over pairs of synsets and compute their path similarity\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.path_similarity(synset2)\n",
    "            if similarity is not None and similarity > best_score:\n",
    "                best_score = similarity\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Returns the sigmoid of x.\"\"\"\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "def definition_avg_similarity(definition, context_word):\n",
    "    \"\"\"\n",
    "    Compute the average similarity of words in the definition to the context word.\n",
    "\n",
    "    :param definition: The definition text.\n",
    "    :param context_word: The context word to compare against.\n",
    "    :param model: Preloaded Word2Vec model.\n",
    "    \"\"\"\n",
    "    tokens = [word for word in definition.lower().split() if word.isalnum()]\n",
    "\n",
    "    total_similarity = 0\n",
    "    valid_token_count = 0\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            similarity = model.similarity(context_word, token)\n",
    "            total_similarity += similarity\n",
    "            valid_token_count += 1\n",
    "\n",
    "    avg_similarity = total_similarity / valid_token_count if valid_token_count else 0\n",
    "\n",
    "    return avg_similarity\n",
    "\n",
    "\n",
    "def context_relevant_definition(word, context_word):\n",
    "    word_synsets = wn.synsets(word)\n",
    "    if not word_synsets:\n",
    "        return None\n",
    "\n",
    "    best_definition = None\n",
    "    max_avg_similarity = -1\n",
    "    for w_syn in word_synsets:\n",
    "        current_avg_similarity = definition_avg_similarity(w_syn.definition(), context_word)\n",
    "        print(f\"{context_word} Definition: '{w_syn.definition()}' has average similarity: {current_avg_similarity}\")\n",
    "        if current_avg_similarity > max_avg_similarity:\n",
    "            max_avg_similarity = current_avg_similarity\n",
    "            best_definition = w_syn.definition()\n",
    "\n",
    "    return best_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9905bd-0e85-4cde-b6e9-9f1cfcbff091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Can', 'could', 0.37111154198646545), ('honeybee', 'sexual', 0.22620375454425812), ('abuse', '##s', 0.5341664552688599), ('lead', 'lead', 0.8986830711364746), ('sting', 'successful', 0.1102379709482193), ('operation', 'attack', 0.16788507997989655)]\n",
      "[('sting', 'successful', 0.1102379709482193), ('operation', 'attack', 0.16788507997989655), ('honeybee', 'sexual', 0.22620375454425812)]\n",
      "Sentence to detect pun: Can honeybee abuse lead to a sting operation ?\n",
      "This is a pun! The word 'sting' has multiple meanings.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'test2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-05f25c924200>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can honeybee abuse lead to a sting operation ?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-05f25c924200>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# for index, item in enumerate(filtered_data):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#     print(f\"Definition {index + 1}: {context_relevant_definition(pun_word, item[0])}. \", end=\"\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mtest2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmax_similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m# print(max_similarity(pun_word[0], item[0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'test2'"
     ]
    }
   ],
   "source": [
    "def main(sentence):\n",
    "    tokenizer = MWETokenizer()  # Multi-word expression tokenizer\n",
    "    tokens = tokenizer.tokenize(word_tokenize(sentence))\n",
    "    # definition = context_relevant_definition(word, context_words)\n",
    "\n",
    "    predictions = [x for x in mask_and_predict(\" \".join(tokens))]\n",
    "    print(predictions)\n",
    "    predictions.sort(key=lambda x: x[2], reverse=False)\n",
    "    filtered_data = predictions[:3]\n",
    "    print(filtered_data)\n",
    "    if filtered_data[0][2] < 0.2:\n",
    "        print(f\"Sentence to detect pun: {sentence}\")\n",
    "        # find element the lowest probability\n",
    "        pun_word = min(filtered_data, key=lambda x: x[2])\n",
    "        if len(wn.synsets(pun_word[0])) <2:\n",
    "            pun_word = filtered_data[1]\n",
    "        filtered_data.remove(pun_word)\n",
    "        # pun_word = pun_word[0]\n",
    "        print(f\"This is a pun! The word '{pun_word[0]}' has multiple meanings.\")\n",
    "        if len(wn.synsets(pun_word[0])) == 2:\n",
    "            for index, definition in enumerate(wn.synsets(pun_word[0])):\n",
    "                print(f\"Definition {index + 1}: {definition.definition()}. \", end=\"\")\n",
    "                print(' ')\n",
    "\n",
    "            return\n",
    "        # for index, item in enumerate(filtered_data):\n",
    "        #     print(f\"Definition {index + 1}: {context_relevant_definition(pun_word, item[0])}. \", end=\"\")\n",
    "        from test2 import max_similarity\n",
    "        for index, item in enumerate(filtered_data):\n",
    "            # print(max_similarity(pun_word[0], item[0]))\n",
    "            print(f\"Definition {index + 1}: {max_similarity(pun_word[0], item[0])[0][0]}. \", end=\"\")\n",
    "            print(' ')\n",
    "\n",
    "    else:\n",
    "        print(\"This is not a pun!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(\"Can honeybee abuse lead to a sting operation ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13505a-dd24-4d59-808c-da00be5c5926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
