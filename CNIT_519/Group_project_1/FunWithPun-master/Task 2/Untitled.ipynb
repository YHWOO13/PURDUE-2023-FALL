{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "40b29673-c31d-4f13-a345-33f8c55e96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    task2.py - Task 2: Pun Location using word2vec and cosine similarities\n",
    "    Author: Dung Le (dungle@bennington.edu)\n",
    "    Date: 10/17/2017\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import gensim\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e53b12a-f8e9-4e4b-9dc9-4decc17fb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b5a2b-3c41-4cb0-bace-db6f754932b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39603da1-d84c-42d7-b8e1-02f7b7fdc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_pun_words(sent):\n",
    "    scores = {}\n",
    "    if len(sent) <= 1:\n",
    "        return set(sent[0])\n",
    "    else:\n",
    "        for i in range(len(sent)-1):\n",
    "            for j in range(i+1, len(sent)):\n",
    "                sim_score = model.similarity(sent[i], sent[j])\n",
    "                scores['{0}-{1}'.format(sent[i], sent[j])] = sim_score\n",
    "\n",
    "        if len(scores) >= 5:\n",
    "            top3 = sorted(zip(scores.values(), scores.keys()), reverse=True)[:3]\n",
    "            print('top3',top3)\n",
    "            print(\" \")\n",
    "            poss = [tup[1].split(sep='-') for tup in top3]\n",
    "            print('poss',poss)\n",
    "            print(\" \")\n",
    "            possible_pun_words = set(poss[0] + poss[1] + poss[2])\n",
    "            \n",
    "            print('possible_pun_words',possible_pun_words)\n",
    "            print(\" \")\n",
    "        else:\n",
    "            poss = [pair.split(sep='-') for pair in scores.keys()]\n",
    "            possible_pun_words = set()\n",
    "            for i in range(len(poss)):\n",
    "                possible_pun_words = possible_pun_words.union(set(poss[i]))\n",
    "            \"\"\"\n",
    "            top = sorted(zip(scores.values(), scores.keys()), reverse=True)[:1]\n",
    "            poss = [tup[1].split(sep='-') for tup in top]\n",
    "            possible_pun_words = set(poss[0])\n",
    "            \"\"\"\n",
    "            \n",
    "        return possible_pun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fd12e644-9889-4edb-a394-3a6bc7db2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pun_word(orig_sent, sent):\n",
    "    pun_words = get_possible_pun_words(sent)\n",
    "    largest_index = 0\n",
    "    print('orig_sent',orig_sent)\n",
    "    print('punwords', pun_words)\n",
    "\n",
    "    for w in pun_words:\n",
    "#         pun_word = w\n",
    "        index = orig_sent.index(w)\n",
    "        print('index',index)\n",
    "\n",
    "        if index > largest_index:\n",
    "            largest_index = index\n",
    "\n",
    "    return largest_index + 1\n",
    "#     return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b74afa0b-a6dd-4482-ae40-3390252807c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " <Element 'corpus' at 0x000001F59EF210E0>\n",
      "4 ['blacksmith']\n",
      "7 ['blacksmith', 'daughter']\n",
      "11 ['blacksmith', 'daughter', 'knew']\n",
      "14 ['blacksmith', 'daughter', 'knew', 'forge']\n",
      "15 ['blacksmith', 'daughter', 'knew', 'forge', 'ahead']\n",
      "hom_3\n",
      "original ['she', 'was', 'only', 'a', 'blacksmith', \"'\", 's', 'daughter', ',', 'but', 'she', 'knew', 'how', 'to', 'forge', 'ahead', '.']\n",
      "sentences ['blacksmith', 'daughter', 'knew', 'forge', 'ahead']\n",
      "top3 [(0.19933294, 'daughter-knew'), (0.19241917, 'forge-ahead'), (0.16441028, 'blacksmith-daughter')]\n",
      " \n",
      "poss [['daughter', 'knew'], ['forge', 'ahead'], ['blacksmith', 'daughter']]\n",
      " \n",
      "possible_pun_words {'blacksmith', 'daughter', 'knew', 'ahead', 'forge'}\n",
      " \n",
      "orig_sent ['she', 'was', 'only', 'a', 'blacksmith', \"'\", 's', 'daughter', ',', 'but', 'she', 'knew', 'how', 'to', 'forge', 'ahead', '.']\n",
      "punwords {'blacksmith', 'daughter', 'knew', 'ahead', 'forge'}\n",
      "index 4\n",
      "index 7\n",
      "index 11\n",
      "index 15\n",
      "index 14\n",
      "[16]\n",
      "['hom_3']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load Google's pre-trained Word2Vec model.\n",
    "    # model = gensim.models.KeyedVectors.load_word2vec_format('../sample/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update('.', '?', '-', '\\'', '\\:', ',', '!', '<', '>', '\\\"', '/', '(', ')',\n",
    "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 's', 't', 're', 'm')\n",
    "\n",
    "    # Load dataset from xml file (task 2)\n",
    "    tree = ET.parse('C:\\\\Users\\\\LG\\\\Desktop\\\\github\\\\PURDUE-2023-FALL\\\\CNIT_519\\\\Group_project_1\\\\FunWithPun-master\\\\sample\\\\subtask2-homographic-test3.xml')\n",
    "                     \n",
    "    root = tree.getroot()\n",
    "    print('root\\n',root)\n",
    "    original_sentences = []\n",
    "    sentences = []\n",
    "    text_ids= []\n",
    "    vocab = model.index_to_key\n",
    "\n",
    "    for child in root:\n",
    "        original_sentence = []\n",
    "        text_id = child.attrib['id']\n",
    "        for i in range(len(child)):\n",
    "            original_sentence.append(child[i].text.lower())\n",
    "        original_sentences.append(original_sentence)\n",
    "        text_ids.append(text_id)\n",
    "\n",
    "    for child in root:\n",
    "        sentence = []\n",
    "        for i in range(len(child)):\n",
    "            if child[i].text.lower() not in stop_words and child[i].text.lower() in vocab:\n",
    "                sentence.append(child[i].text.lower())\n",
    "                print(i, sentence)\n",
    "\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    k = 2\n",
    "    print(text_ids[0])\n",
    "    print('original',original_sentences[0])\n",
    "    print('sentences',sentences[0])\n",
    "\n",
    "    get_pun_words_list = []\n",
    "    text_list = []\n",
    "    with open(\"task2_DungLe_top3_all.txt\", \"w\") as file:\n",
    "        for i in range(len(sentences)):\n",
    "#             get_pun_words_result = str(get_pun_word(original_sentences[k], sentences[k])) + \"\\n\"\n",
    "            get_pun_words_result = get_pun_word(original_sentences[i], sentences[i])\n",
    "            get_pun_words_list.append(get_pun_words_result)\n",
    "            text_list.append(text_ids[i])\n",
    "        print(get_pun_words_list)\n",
    "        print(text_list)\n",
    "\n",
    "#     with open(\"task2_DungLe_top3_all.txt\", \"w\") as file:\n",
    "#         for i in range(len(sentences)):\n",
    "#             result = text_ids[i] + \" \" + text_ids[i] + \"_\" + str(get_pun_word(original_sentences[i], sentences[i])) + \"\\n\"\n",
    "#             print(result)\n",
    "#             file.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "659853e4-df3b-4656-b469-73b03e88320a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1607"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_pun_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95656045-2a32-49ed-aa8d-1b7176287a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load Google's pre-trained Word2Vec model.\n",
    "    # model = gensim.models.KeyedVectors.load_word2vec_format('../sample/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update('.', '?', '-', '\\'', '\\:', ',', '!', '<', '>', '\\\"', '/', '(', ')',\n",
    "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 's', 't', 're', 'm')\n",
    "\n",
    "    # Load dataset from xml file (task 2)\n",
    "    tree = ET.parse('C:\\\\Users\\\\LG\\\\Desktop\\\\github\\\\PURDUE-2023-FALL\\\\CNIT_519\\\\Group_project_1\\\\FunWithPun-master\\\\sample\\\\subtask2-homographic-test3.xml')\n",
    "                     \n",
    "    root = tree.getroot()\n",
    "    original_sentences = []\n",
    "    sentences = []\n",
    "    text_ids= []\n",
    "#     vocab = model.vocab.keys()\n",
    "    vocab = model.index_to_key\n",
    "\n",
    "    for child in root:\n",
    "        original_sentence = []\n",
    "        text_id = child.attrib['id']\n",
    "        for i in range(len(child)):\n",
    "            original_sentence.append(child[i].text.lower())\n",
    "        original_sentences.append(original_sentence)\n",
    "        text_ids.append(text_id)\n",
    "\n",
    "    for child in root:\n",
    "        sentence = []\n",
    "        for i in range(len(child)):\n",
    "            if child[i].text.lower() not in stop_words and child[i].text.lower() in vocab:\n",
    "                sentence.append(child[i].text.lower())\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    k = 2\n",
    "    print(text_ids[0])\n",
    "    print(original_sentences[0])\n",
    "    print(sentences[0])\n",
    "    print(text_ids[2])\n",
    "    print(original_sentences[k])\n",
    "    print(sentences[2])\n",
    "    print('get_possible_pun_words', get_possible_pun_words(sentences[k]))\n",
    "    \n",
    "#     get_pun_words_result = text_ids[2] + \" \" + text_ids[2] + \"_\" + str(get_pun_word(original_sentences[2], sentences[2])) + \"\\n\"\n",
    "#     get_pun_words_result = str(get_pun_word(original_sentences[k], sentences[k])) + \"\\n\"\n",
    "\n",
    "#     print('get_pun_words_result', get_pun_words_result)\n",
    "    get_pun_words_list = []\n",
    "    text_list = []\n",
    "    with open(\"task2_DungLe_top3_all.txt\", \"w\") as file:\n",
    "        for i in range(len(sentences)):\n",
    "            \n",
    "#             get_pun_words_result = str(get_pun_word(original_sentences[k], sentences[k])) + \"\\n\"\n",
    "            get_pun_words_result = get_pun_word(original_sentences[i], sentences[i])\n",
    "            get_pun_words_list.append(get_pun_words_result)\n",
    "            text_list.append(text_ids[i])\n",
    "        print(get_pun_words_list)\n",
    "        print(text_list)\n",
    "\n",
    "#     with open(\"task2_DungLe_top3_all.txt\", \"w\") as file:\n",
    "#         for i in range(len(sentences)):\n",
    "#             result = text_ids[i] + \" \" + text_ids[i] + \"_\" + str(get_pun_word(original_sentences[i], sentences[i])) + \"\\n\"\n",
    "#             print(result)\n",
    "#             file.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ace05a0-5beb-42f3-87cd-751cf0f03840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_with_most_senses():\n",
    "    for child in root:\n",
    "        senses_number = 0\n",
    "        prev_word_id = \"\"\n",
    "        text_id = child.attrib['id']\n",
    "#         print('text_id', text_id)\n",
    "        for i in range(len(child)):\n",
    "            if child[i].text.lower() not in stop_words:\n",
    "                word_senses_num = len(wn.synsets(child[i].text.lower()))\n",
    "                print(\" \")\n",
    "                print('wordsenses_num',word_senses_num)\n",
    "                print('synsets', wn.synsets(child[i].text.lower()))\n",
    "                current_word_id = child[i].attrib['id']\n",
    "                print('current_word_id',current_word_id)\n",
    "                if word_senses_num >= senses_number:\n",
    "                    senses_number = word_senses_num\n",
    "                    prev_word_id = current_word_id\n",
    "        pun_prediction = text_id + \" \" + prev_word_id + \"\\n\"\n",
    "        print(pun_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "825589ac-7f65-4503-b977-862dc0f3e48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "wordsenses_num 1\n",
      "synsets [Synset('blacksmith.n.01')]\n",
      "current_word_id hom_5_5\n",
      " \n",
      "wordsenses_num 1\n",
      "synsets [Synset('daughter.n.01')]\n",
      "current_word_id hom_5_8\n",
      " \n",
      "wordsenses_num 11\n",
      "synsets [Synset('know.v.01'), Synset('know.v.02'), Synset('know.v.03'), Synset('know.v.04'), Synset('know.v.05'), Synset('acknowledge.v.06'), Synset('know.v.07'), Synset('sleep_together.v.01'), Synset('know.v.09'), Synset('know.v.10'), Synset('know.v.11')]\n",
      "current_word_id hom_5_12\n",
      " \n",
      "wordsenses_num 9\n",
      "synsets [Synset('forge.n.01'), Synset('forge.n.02'), Synset('forge.v.01'), Synset('forge.v.02'), Synset('invent.v.01'), Synset('forge.v.04'), Synset('forge.v.05'), Synset('shape.v.02'), Synset('fashion.v.01')]\n",
      "current_word_id hom_5_15\n",
      " \n",
      "wordsenses_num 8\n",
      "synsets [Synset('ahead.s.01'), Synset('ahead.r.01'), Synset('ahead.r.02'), Synset('ahead.r.03'), Synset('ahead.r.04'), Synset('ahead.r.05'), Synset('ahead.r.06'), Synset('ahead.r.07')]\n",
      "current_word_id hom_5_16\n",
      "hom_3 hom_5_12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_word_with_most_senses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b2c54-d7b2-4185-a61c-7889eafa147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_pun_words {'blacksmith', 'daughter', 'knew', 'ahead', 'forge'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
