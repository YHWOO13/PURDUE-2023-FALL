{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e04bc3c-7a89-4e58-98ae-666a1781d96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk import word_tokenize, MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22e84b1-c857-415a-ab09-624c48c4ae89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'downlaod'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c2883bdb94a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownlaod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'downlaod'"
     ]
    }
   ],
   "source": [
    "nltk.downlaod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d23a99d-670e-472a-a918-dccd224cddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_predict(sentence, top_n=3):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    words = sentence.split()\n",
    "    results = []\n",
    "\n",
    "    # Loop through each word in the sentence\n",
    "    for i in range(len(words)):\n",
    "        masked_sentence = ' '.join([words[j] if j != i else '[MASK]' for j in range(len(words))])\n",
    "        inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "\n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "            logits = output.logits\n",
    "\n",
    "        # Get top_n predictions for the masked word\n",
    "        masked_index = torch.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)[0].item()\n",
    "        probs = logits[0, masked_index].softmax(dim=0)\n",
    "        top_prob_values, top_indices = torch.topk(probs, top_n)\n",
    "\n",
    "        print(f\"Original Word: {words[i]}\")\n",
    "        for idx, token_id in enumerate(top_indices):\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
    "            prob_value = top_prob_values[idx].item()\n",
    "            print(f\"   Candidate {idx + 1}: {predicted_token} with probability {prob_value:.4f}\")\n",
    "\n",
    "        top_prediction = tokenizer.convert_ids_to_tokens([top_indices[0].item()])[0]\n",
    "        results.append((words[i], top_prediction, top_prob_values[0].item()))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95e52dea-cff7-4927-be99-caadcf85b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f1d6537-0f94-41e7-979f-2e3e79da0bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: If\n",
      "   Candidate 1: if with probability 0.6204\n",
      "   Candidate 2: when with probability 0.2666\n",
      "   Candidate 3: unless with probability 0.0627\n",
      "Original Word: you\n",
      "   Candidate 1: you with probability 0.9910\n",
      "   Candidate 2: i with probability 0.0039\n",
      "   Candidate 3: they with probability 0.0020\n",
      "Original Word: burn\n",
      "   Candidate 1: light with probability 0.1434\n",
      "   Candidate 2: hold with probability 0.1374\n",
      "   Candidate 3: see with probability 0.0661\n",
      "Original Word: the\n",
      "   Candidate 1: a with probability 0.7958\n",
      "   Candidate 2: the with probability 0.1513\n",
      "   Candidate 3: your with probability 0.0157\n",
      "Original Word: candle\n",
      "   Candidate 1: fuse with probability 0.0471\n",
      "   Candidate 2: wires with probability 0.0209\n",
      "   Candidate 3: skin with probability 0.0163\n",
      "Original Word: on\n",
      "   Candidate 1: on with probability 0.4693\n",
      "   Candidate 2: at with probability 0.4605\n",
      "   Candidate 3: from with probability 0.0213\n",
      "Original Word: both\n",
      "   Candidate 1: both with probability 0.6140\n",
      "   Candidate 2: the with probability 0.1457\n",
      "   Candidate 3: your with probability 0.0884\n",
      "Original Word: ends\n",
      "   Candidate 1: sides with probability 0.7942\n",
      "   Candidate 2: ends with probability 0.0592\n",
      "   Candidate 3: hands with probability 0.0254\n",
      "Original Word: ,\n",
      "   Candidate 1: , with probability 0.9780\n",
      "   Candidate 2: then with probability 0.0084\n",
      "   Candidate 3: . with probability 0.0023\n",
      "Original Word: you\n",
      "   Candidate 1: they with probability 0.8577\n",
      "   Candidate 2: you with probability 0.1194\n",
      "   Candidate 3: we with probability 0.0171\n",
      "Original Word: ’\n",
      "   Candidate 1: ' with probability 0.9998\n",
      "   Candidate 2: \" with probability 0.0001\n",
      "   Candidate 3: , with probability 0.0000\n",
      "Original Word: re\n",
      "   Candidate 1: are with probability 0.2151\n",
      "   Candidate 2: still with probability 0.0989\n",
      "   Candidate 3: be with probability 0.0973\n",
      "Original Word: not\n",
      "   Candidate 1: not with probability 0.3779\n",
      "   Candidate 2: just with probability 0.1392\n",
      "   Candidate 3: almost with probability 0.0660\n",
      "Original Word: as\n",
      "   Candidate 1: as with probability 0.9933\n",
      "   Candidate 2: so with probability 0.0034\n",
      "   Candidate 3: very with probability 0.0004\n",
      "Original Word: bright\n",
      "   Candidate 1: lucky with probability 0.1217\n",
      "   Candidate 2: bad with probability 0.0723\n",
      "   Candidate 3: strong with probability 0.0549\n",
      "Original Word: as\n",
      "   Candidate 1: as with probability 0.9938\n",
      "   Candidate 2: than with probability 0.0054\n",
      "   Candidate 3: like with probability 0.0002\n",
      "Original Word: you\n",
      "   Candidate 1: you with probability 0.9428\n",
      "   Candidate 2: i with probability 0.0196\n",
      "   Candidate 3: they with probability 0.0173\n",
      "Original Word: think\n",
      "   Candidate 1: look with probability 0.4562\n",
      "   Candidate 2: think with probability 0.1517\n",
      "   Candidate 3: thought with probability 0.0991\n",
      "Original Word: .\n",
      "   Candidate 1: . with probability 0.9773\n",
      "   Candidate 2: ; with probability 0.0148\n",
      "   Candidate 3: ! with probability 0.0075\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"If you burn the candle on both ends, you’re not as bright as you think.\"\"\"\n",
    "tokenizer = MWETokenizer()  # Multi-word expression tokenizer\n",
    "tokens = tokenizer.tokenize(word_tokenize(sentence))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
    "# print(\" \".join(filtered_tokens))\n",
    "predictions = mask_and_predict(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c8313-8b3c-4da8-8919-47794575573c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
